{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe393dc-de89-4e23-8c11-d8340af07e42",
   "metadata": {},
   "source": [
    "# 1. Intro\n",
    "\n",
    "For an intro and more context about this project click <a href=\"https://github.com/4rn3/rl_school_project#readme\">here<a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682b9d5-46b2-44bd-a24d-c5bd013ac8ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Algorithms & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3b9ad-aad9-4c8c-b7f5-1415f664ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random\n",
    "from collections import deque\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation, Monitor\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout, Input, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "random.seed(1337)\n",
    "RANDOM_SEED = 1337\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "tf.compat.v1.GPUOptions(allow_growth=True) #try to avoid out of mem error\n",
    "\n",
    "print(gym.__version__)\n",
    "print(tf.__version__) #2.10 last version to support GPU on win\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a4468-c00b-4836-9eba-e67b25640784",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"SpaceInvaders-v4\")\n",
    "env = GrayScaleObservation(env,keep_dim=True)\n",
    "env = ResizeObservation(env, (84,84)) #resize\n",
    "env = FrameStack(env, 4) #gain extra info by combining multiple frames i.e is mario landing or jumping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c304d71-63e4-4614-82a0-bbcc945f141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Observation space: \", env.observation_space.shape)\n",
    "print(\"# Actions: \", env.action_space.n)\n",
    "print(\"Available actions: \", env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ea87e-b1c6-4325-a819-997e6f57d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked, height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e01b4b-61bc-44f0-a87a-88b72ca07423",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_screen = env.render(mode = 'rgb_array')\n",
    "plt.imshow(env_screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558cf5fe-b24d-4eca-beef-2a5a3f0e3444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(values, title=''):   \n",
    "    ''' Plot the reward curve and histogram of results over time.'''\n",
    "   \n",
    "    # Define the figure\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    f.suptitle(title)\n",
    "    ax[0].plot(values, label='score per run')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    x = range(len(values))\n",
    "    ax[0].legend()\n",
    "    # Calculate the trend\n",
    "    try:\n",
    "        z = np.polyfit(x, values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
    "    except:\n",
    "        print('')\n",
    "    \n",
    "    # Plot the histogram of results\n",
    "    ax[1].hist(values[-50:])\n",
    "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].legend()\n",
    "    plt.savefig(f\"./plots/{title}.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806429df-b24b-4af0-82e4-1e808bd821ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.1 DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5449f4-d89f-424b-91dc-9235042f498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyp\n",
    "TRAIN_EPISODE = 100\n",
    "TEST_EPISODE = 10\n",
    "LR = 1e-4\n",
    "MEM_LEN = 50000\n",
    "BATCH_SIZE = 64\n",
    "UPDATE_TARGET_TRESHOLD = 5\n",
    "AGGREGATE_STATS = 5\n",
    "MODEL_NAME = \"DQN\"\n",
    "MIN_REPLAY_SIZE = 1000\n",
    "\n",
    "#Bellman hyp\n",
    "EPSILON = 1\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "DECAY = 0.03\n",
    "BEL_LR = 0.7\n",
    "DISCOUNT = 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bbca84-42f2-4546-bc7b-7a2d9501d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state_shape, action_shape):\n",
    "    \n",
    "    model = Sequential()\n",
    "    #Feature generation\n",
    "    model.add(Conv2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(stacked, height, width, channels)))\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "    #Dim reduction\n",
    "    model.add(Flatten())\n",
    "    #FC classifier\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LR), metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940bc53b-660d-4f04-9200-7527cc9cb7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = agent(env.observation_space.shape, env.action_space.n) #init prediction model\n",
    "target_model = agent(env.observation_space.shape, env.action_space.n) #init target model\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "replay_memory = deque(maxlen=MEM_LEN) #init memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6326c5-3351-4927-9b13-81940b933696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(env, replay_memory, model, target_model, done):\n",
    "    learning_rate = BEL_LR\n",
    "    discount_factor = DISCOUNT\n",
    "    \n",
    "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
    "        return\n",
    "    batch_size = BATCH_SIZE\n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "    \n",
    "    #Predicting the Q-values\n",
    "    current_states = np.array([transition[0] for transition in mini_batch]) #add current_state from minibatch\n",
    "    current_qs_list = model.predict(current_states, verbose=0)\n",
    "\n",
    "    new_current_states = np.array([transition[3]for transition in mini_batch]) #add new_current_state from minibatch\n",
    "    future_qs_list = target_model.predict(new_current_states, verbose=0)\n",
    "  \n",
    "    X, y = [], []\n",
    "\n",
    "   #Updating Q-values\n",
    "    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
    "        if not done:\n",
    "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "            \n",
    "        #Bellman\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
    "        \n",
    "        X.append(observation)\n",
    "        y.append(current_qs)\n",
    "    \n",
    "    model.fit(np.array(X), np.array(y), batch_size=batch_size, verbose=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fddfd5-533e-4828-bbf3-7b501025ec0c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Train loop\n",
    "X, y, Total_rewards = [], [], []\n",
    "highest_reward = 150\n",
    "steps_to_update_target_model = 0\n",
    "time_tracking = []\n",
    "\n",
    "for episode in trange(TRAIN_EPISODE):\n",
    "    n_steps_episode = 0\n",
    "    total_training_rewards = 0\n",
    "\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    start = time.time()\n",
    "    while not done:\n",
    "        #track time\n",
    "        \n",
    "        steps_to_update_target_model += 1\n",
    "        random_number = np.random.rand()\n",
    "        #Exploration\n",
    "        if random_number <= EPSILON:\n",
    "            action = env.action_space.sample()\n",
    "        #Exploitation    \n",
    "        else:\n",
    "            predicted = model.predict(np.array(observation), verbose=0).flatten()\n",
    "            action = np.argmax(predicted)\n",
    "\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        total_training_rewards += reward\n",
    "        \n",
    "        replay_memory.append([observation, action, reward, new_observation, done])\n",
    "\n",
    "        #Update prediction model\n",
    "        if steps_to_update_target_model % 4 == 0 or done:\n",
    "            batch_train(env, replay_memory, model, target_model, done)\n",
    "        \n",
    "        observation = new_observation\n",
    "        \n",
    "        n_steps_episode += 1\n",
    "        \n",
    "        if done:\n",
    "            stop = time.time()\n",
    "            time_tracking.append(stop-start)\n",
    "            \n",
    "            print('{} Total training rewards: {} after n steps = {}'.format(episode, total_training_rewards, n_steps_episode))\n",
    "            Total_rewards.append(total_training_rewards)\n",
    "\n",
    "            #Update target model\n",
    "            if steps_to_update_target_model >= UPDATE_TARGET_TRESHOLD:\n",
    "                target_model.set_weights(model.get_weights())\n",
    "                steps_to_update_target_model = 0\n",
    "            break\n",
    "    \n",
    "    #Update epsilon \n",
    "    EPSILON = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY * episode)\n",
    "    \n",
    "    avg_score = round(np.mean(Total_rewards[-100:]),2)\n",
    "    \n",
    "    #Save model and metrics\n",
    "    if (episode % AGGREGATE_STATS == 0) or (total_training_rewards > highest_reward):\n",
    "        print(\"Saving model\")\n",
    "        avg_time = round(sum(time_tracking)/len(time_tracking),2) #avg time for aggregate amount of episodes\n",
    "        \n",
    "        model.save(f'./models/DQN_models/{str(episode)}_{MODEL_NAME}.h5')\n",
    "        \n",
    "        with open(f\"./logs/DQN_logs/DQN.txt\",\"a\") as f:\n",
    "            f.write(f\"{avg_score},{total_training_rewards},{avg_time},{str(episode)},{n_steps_episode}\\n\")\n",
    "        \n",
    "        time_tracking = []\n",
    "        \n",
    "        if total_training_rewards > highest_reward:\n",
    "            highest_reward = total_training_rewards\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b279ee1-37e3-4b83-b3c5-27074c67e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot trainig\n",
    "plot_res(Total_rewards,'DQN_train4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ce2fd-594c-468b-bfcf-bcac64600d3b",
   "metadata": {},
   "source": [
    "### 2.1.1 Log analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d35a56-0fa2-49c1-84bf-e2fc8af2c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Highest average rewards\n",
    "dqn_df = pd.read_csv('./logs/DQN_logs/DQN4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba811e-102a-494d-aefb-8d3e7138a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_df.sort_values(by=[\"avg_score\",\"episode\"], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d678bdb-1e83-4577-9182-96168f7589cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#highest max reward (influecned by steps)\n",
    "dqn_df.sort_values(by=[\"total_training_rewards\",\"episode\"], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99354eba-47f9-4864-b8bf-c2d096f53a56",
   "metadata": {},
   "source": [
    "### 2.1.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45e899-e938-41d8-b717-4b6a5c587e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load models\n",
    "best_dqn_model = \"76_DQN.h5\"\n",
    "best_dqn_model = load_model(os.path.join(\"./models/DQN_models\",best_dqn_model), compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73496b-8092-4f3f-bf99-14b209b9481f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Evaluate model\n",
    "env = gym.make(\"SpaceInvaders-v4\")\n",
    "env = GrayScaleObservation(env,keep_dim=True)\n",
    "env = ResizeObservation(env, (84,84))\n",
    "env = FrameStack(env, 4)\n",
    "\n",
    "Total_reward = []\n",
    "for episode in trange(TEST_EPISODE):\n",
    "        n_steps_episode = 0\n",
    "        total_training_rewards = 0\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            observation = tf.convert_to_tensor(observation)\n",
    "            observation = tf.expand_dims(observation, 0)\n",
    "            \n",
    "            predicted = best_dqn_model.predict(observation, verbose=0)\n",
    "            action = np.argmax(predicted)\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            total_training_rewards += reward  \n",
    "            observation = new_observation\n",
    "            \n",
    "            n_steps_episode += 1\n",
    "        print('{} Reward = {}'.format(episode,total_training_rewards))\n",
    "        Total_reward.append(total_training_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb489c7-1357-4fe2-8876-f0010e44c00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_res(Total_reward,'DQN_test2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d223e-bc8d-4f72-8bd1-ce8f5bc898d5",
   "metadata": {},
   "source": [
    "# 2.2 Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f010bae-0453-4ba0-9a6e-803822f5dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyp\n",
    "TRAIN_EPISODE = 100\n",
    "TEST_EPISODE = 10\n",
    "LR = 1e-4\n",
    "GOAL = 1000\n",
    "AGGREGATE_STATS = 5\n",
    "MAX_STEPS_PER_EPISODEE = 30000\n",
    "MODEL_NAME = \"AC\"\n",
    "GAMMA = 0.99 \n",
    "EPS = np.finfo(np.float32).eps.item() #smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bbbb24-a193-4135-91d7-dbb1d9b8992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input\n",
    "inputs = Input((height, width, channels))\n",
    "\n",
    "#Feature generation\n",
    "features = Conv2D(32, (8,8), strides=(4,4), activation='relu')(inputs)\n",
    "features = Conv2D(64, (4,4), strides=(2,2), activation='relu')(features)\n",
    "features = Conv2D(128, (3,3), activation='relu')(features)\n",
    "\n",
    "#Dim reduction\n",
    "flatten = Flatten()(features)\n",
    "\n",
    "fc1 = Dense(512, activation='relu')(flatten)\n",
    "fc2 = Dense(256, activation='relu')(fc1)\n",
    "\n",
    "actor = Dense(env.action_space.n, activation=\"softmax\")(fc2)\n",
    "critic = Dense(1)(fc2)\n",
    "\n",
    "# Construct the model\n",
    "model = keras.Model(inputs=inputs, outputs=[actor, critic])\n",
    "print(model.summary())\n",
    "\n",
    "# Add adam optimizer and Huber loss\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LR)\n",
    "huber_loss = keras.losses.Huber()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=huber_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a60675-80cb-47d1-a040-bc356d00f2a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "score_history = []\n",
    "\n",
    "highest_reward = 150\n",
    "\n",
    "average_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "\n",
    "#while True:\n",
    "for i in trange(TRAIN_EPISODE):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    time_tracking = []\n",
    "    with tf.GradientTape() as tape:\n",
    "        start_time = time.time()\n",
    "        for timestep in range(1, MAX_STEPS_PER_EPISODEE):\n",
    "            \n",
    "            state = tf.convert_to_tensor(state)\n",
    "            \n",
    "            action_probs, critic_value = model(state)\n",
    "            \n",
    "            p = np.squeeze(action_probs)\n",
    "            \n",
    "            action = np.random.choice(env.action_space.n, p=p[0])\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "            \n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            state, reward, done, info = env.step(action)\n",
    "            \n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                stop_time = time.time()\n",
    "                time_tracking.append(stop_time-start_time)\n",
    "                break\n",
    "        \n",
    "        score_history.append(episode_reward)\n",
    "                \n",
    "        # Calculate expected value from rewards\n",
    "        G = np.zeros_like(rewards_history)\n",
    "        for t in range(len(rewards_history)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(rewards_history)):\n",
    "                G_sum += rewards_history[k]*discount\n",
    "                discount *= GAMMA\n",
    "            G[t] = G_sum\n",
    "        \n",
    "        # normalize\n",
    "        G = np.array(G)\n",
    "        G = (G - np.mean(G)) / (np.std(G) + EPS)\n",
    "        G = G.tolist()\n",
    "        \n",
    "        history = zip(action_probs_history, critic_value_history, G)\n",
    "\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        \n",
    "        for log_prob, value, rew in history:\n",
    "            diff = rew - value\n",
    "            #update actor\n",
    "            actor_losses.append(-log_prob * diff) # actor loss\n",
    "            #update critic\n",
    "            critic_losses.append(huber_loss(tf.expand_dims(value, 0), tf.expand_dims(rew, 0)))\n",
    "        \n",
    "        ## Combine losses\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        \n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "        \n",
    "\n",
    "    episode_count += 1\n",
    "    \n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "     #Save model and metrics\n",
    "    if (episode_count % AGGREGATE_STATS == 0) or (episode_reward > highest_reward):\n",
    "        print(f\"episode: {episode_count}, avg score: {round(avg_score,2)}\")\n",
    "        avg_time = round(sum(time_tracking)/len(time_tracking),2) #avg time for aggregate amount of episodes\n",
    "        \n",
    "        model.save(f'./models/AC_models/{str(episode_count)}_{MODEL_NAME}.h5')\n",
    "        \n",
    "        with open(f\"./logs/AC_logs/AC.txt\",\"a\") as f:\n",
    "            f.write(f\"{round(avg_score,2)},{episode_reward},{avg_time},{episode_count}\\n\")\n",
    "            \n",
    "        if episode_reward > highest_reward:\n",
    "            highest_reward = episode_reward \n",
    "            \n",
    "    if avg_score > GOAL:\n",
    "        model.save(f'./models/AC_models/{str(episode_count)}_{MODEL_NAME}.h5')\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66babca9-8fa6-4be3-ab0f-86286c8b3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_res(score_history,'AC_train_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00820c8-a683-4a40-aefc-f325748bddeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2.1 Log analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea258de1-2d9f-41bf-8a0b-895c7bc2b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_df = pd.read_csv('./logs/AC_logs/AC5.csv')\n",
    "ac_df.sort_values(by=[\"avg_score\", \"episode_reward\"], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f8c7d-1025-40d4-aa7a-836f164d938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_df.sort_values(by=[\"episode_reward\",\"avg_score\"], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05ae84-ec82-4103-9b5e-307dae385c69",
   "metadata": {},
   "source": [
    "### 2.2.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e62a5c-aa3e-4ff7-801c-23aae6b36ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the best model\n",
    "best_ac_model = \"41_AC.h5\"\n",
    "best_ac_model = load_model(os.path.join(\"./models/AC_models\",best_ac_model), compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427134fd-67ca-408e-b6d1-f5df9f2ec0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"SpaceInvaders-v4\")\n",
    "env = GrayScaleObservation(env,keep_dim=True)\n",
    "env = ResizeObservation(env, (84,84))\n",
    "env = FrameStack(env, 4)\n",
    "\n",
    "score_history = []\n",
    "n_episodes = 10\n",
    "\n",
    "for i in trange(n_episodes):\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            observation = tf.convert_to_tensor(observation)\n",
    "            \n",
    "            action_probs, critic_value = best_ac_model(observation)\n",
    "            p = np.squeeze(action_probs)\n",
    "\n",
    "            action = np.random.choice(env.action_space.n, p=p[0])\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "        \n",
    "        score_history.append(score)\n",
    "        \n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        print('episode ', i,  'avg score %.1f' % avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5587d-1e31-4c48-9a03-852c0c55b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_res(score_history,'AC_test_100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf2e47-089c-40f6-8b59-467b8b4995bd",
   "metadata": {},
   "source": [
    "### 3 Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1f0ec-1ae6-4763-a6ff-3f75c1dec4bb",
   "metadata": {},
   "source": [
    "### 3.1 Models compared to random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570a2b6-9284-43a8-b4b4-44947a42f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#play 1 game with random actions\n",
    "env = gym.make(\"SpaceInvaders-v4\")\n",
    "env = GrayScaleObservation(env,keep_dim=True)\n",
    "env = ResizeObservation(env, (84,84))\n",
    "env = FrameStack(env, 4)\n",
    "env = Monitor(env, \"./recordings/\", force=True)\n",
    "\n",
    "epochs, rndm_rewards = 0, 0\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    #env.render()\n",
    "    rndm_rewards  += reward\n",
    "    epochs += 1\n",
    "    \n",
    "env.close()\n",
    "print(f\"Reward: {rndm_rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf6fe0-0c81-4915-9f0a-621514c03467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#play 1 game of the dqn best model\n",
    "env = gym.make(\"SpaceInvaders-v4\")\n",
    "env = GrayScaleObservation(env,keep_dim=True)\n",
    "env = ResizeObservation(env, (84,84))\n",
    "env = FrameStack(env, 4)\n",
    "env = Monitor(env, \"./recordings/\", force=True)\n",
    "\n",
    "epochs, dqn_rewards = 0, 0\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    observation = tf.convert_to_tensor(observation)\n",
    "    observation = tf.expand_dims(observation, 0)\n",
    "    \n",
    "    predicted = best_dqn_model.predict(np.array(observation), verbose=0).flatten()\n",
    "    action = np.argmax(predicted)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    #env.render()\n",
    "    dqn_rewards  += reward\n",
    "    epochs += 1\n",
    "    \n",
    "env.close()\n",
    "print(f\"Reward: {dqn_rewards}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b1c86-cce8-4a9c-a690-96403904a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#play 1 game of the ac best model\n",
    "env = gym.make(\"SpaceInvaders-v4\")\n",
    "env = GrayScaleObservation(env,keep_dim=True)\n",
    "env = ResizeObservation(env, (84,84))\n",
    "env = FrameStack(env, 4)\n",
    "env = Monitor(env, \"./recordings/\", force=True)\n",
    "\n",
    "epochs, ac_rewards = 0, 0\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    observation = tf.convert_to_tensor(observation)\n",
    "            \n",
    "    action_probs, critic_value = best_ac_model(observation)\n",
    "    p = np.squeeze(action_probs)\n",
    "\n",
    "    action = np.random.choice(env.action_space.n, p=p[0])\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    #env.render()\n",
    "    ac_rewards += reward\n",
    "    \n",
    "env.close()\n",
    "print(f\"Reward: {ac_rewards}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f10255-5d4f-48fc-88f3-4125201dc44e",
   "metadata": {},
   "source": [
    "### 3.2 Anaylitical comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2df49e-28de-448e-9aac-1f0a8838e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DQN Highest avg score: \", dqn_df[dqn_df[\"episode\"] > 50].sort_values(by=[\"avg_score\",\"episode\"], ascending=False)['avg_score'].iloc[0])\n",
    "print(\"AC Highest avg score: \", ac_df.sort_values(by=[\"avg_score\", \"episode_reward\"], ascending=False)['avg_score'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc8b69-0379-4455-bd13-47cb7f246419",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DQN Highest highest ep score: \", dqn_df.sort_values(by=[\"total_training_rewards\",\"episode\"], ascending=False)['total_training_rewards'].iloc[0])\n",
    "print(\"AC Highest highest ep score: \", ac_df.sort_values(by=[\"episode_reward\",\"epsiode_count\"], ascending=False)['episode_reward'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f273133-49e5-42a9-a51c-82bf8fd7a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max & AVG rewards per episode \n",
    "fig, axs = plt.subplots(2,2, figsize=(10, 10))\n",
    "axs[0][0].plot(dqn_df[\"episode\"], dqn_df[\"total_training_rewards\"], label=\"DQN max ep reward\")\n",
    "axs[0][0].legend(loc=\"upper left\")\n",
    "axs[0][1].plot(dqn_df[dqn_df[\"episode\"] > 50][\"episode\"], dqn_df[dqn_df[\"episode\"] > 50][\"avg_score\"], label=\"DQN avg reward total\")\n",
    "axs[0][1].legend(loc=\"lower right\")\n",
    "\n",
    "axs[1][0].plot(ac_df[\"epsiode_count\"], ac_df[\"episode_reward\"], label=\"AC max ep reward\")\n",
    "axs[1][0].legend(loc=\"upper right\")\n",
    "axs[1][1].plot(ac_df[\"epsiode_count\"], ac_df[\"avg_score\"], label=\"AC avg reward total\")\n",
    "axs[1][1].legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872958c-f3e3-461a-9054-18226de8bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_time = dqn_df[dqn_df[\"episode\"] % 5 ==0]\n",
    "ac_time = ac_df[ac_df[\"epsiode_count\"] % 5 ==0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, figsize=(10, 10))\n",
    "ax1.plot(dqn_time[\"avg_time\"], label=\"DQN avg time\")\n",
    "ax1.legend(loc=\"best\")\n",
    "ax2.plot(ac_time[\"avg_time\"], label=\"AC avg time\")\n",
    "ax2.legend(loc=\"best\")\n",
    "\n",
    "ac_time[\"avg_time\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d8e01-a4e0-4c81-8961-f444a5da341d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
